\chapter{Basic Visual Computing with OpenCV}
\label{chap:basicvisualcomputing}
In this thesis the \textit{Qt-creator} IDE is used in combination with \textit{ROS-KINETIC} and its \textit{OpenCV}-library to build the calibration program. In order to access the source code of it a short tutorial on how to install the IDE, ROS-Kinetic and additional drivers is explained in the \textit{README} file located in the project folder. Based on the \textit{OpenCV}-library some core elements of visual computing and their implementation in the library are explained in the following. 

\section{Image Containers}

\begin{figure}[h]
\centering
	\subfloat[IMREAD\_COLOR]{%
    \includegraphics[width=.30\textwidth]{pictures/basics/imread/apple_color.png}}\hspace{1cm}
    \subfloat[IMREAD\_GRAYSCALE]{%
    \includegraphics[width=.30\textwidth]{pictures/basics/imread/apple_gray.png}}\\
    \subfloat[zoomed color]{%
    \includegraphics[width=.30\textwidth]{pictures/basics/imread/appletipzoomed_color.png}}\hspace{1cm}
  	\subfloat[zoomed grayscale]{%
    \includegraphics[width=.30\textwidth]{pictures/basics/imread/appl_colot_tip_zomed.png}}\\
   \caption{Images loaded with \textit{OpenCV} with different flags and a zoomed in version of both}
    \label{fig:imread}
    \end{figure}


Digital acquired images are saved into image containers which are basically matrices. The amount of matrices in which an image is saved are called \textit{channels}. For example an image which is saved only into a single channel is called a \textit{grayscale} image in which the value of each pixel is a single sample representing a intensity information. How fine this intensity information of each pixel is saved depends on the \textit{bit depth} which represents the available number space of each entry in the matrix. Images with color information require usually three channels red, blue and green with a bit depth of 8-bit so the light intensity values of each pixel can be divided into $2^8=256$ gradations.

With these three channels a palette of 16,777,216 colors can be produced. Adding several channels to an image expands each of its column with the amount of additional channels times the already existing column. This is illustrated above with an image of $3\times3$ pixels displaying a vertical white line.
\begin{figure}[h]
\begin{tasks}(2)
\task
$
\begin{bmatrix}
  0 & 255 & 0 \\ 
  0 & 255 & 0 \\
  0 & 255 & 0
\end{bmatrix}
$
\task
$
\begin{bmatrix}
  0 & 0 & 0 & 255 & 255 & 255 & 0 & 0 & 0 \\ 
  0 & 0 & 0 & 255 & 255 & 255 & 0 & 0 & 0 \\
  0 & 0 & 0 & 255 & 255 & 255 & 0 & 0 & 0
\end{bmatrix}
$
\end{tasks}
\caption{Representation of a $3\times3$ pixel image with a vertical white line in the middle, $a)$ in grayscale and $b)$ in a 3 channel a color image, with a bit depth of eight}
\end{figure}

\vspace{0.4cm}


In \textit{OpenCV} the image containers are implemented as a \textit{Mat} object. A \textit{Mat} object has two parameters, its  size and the color space and data type used. 
Loading an image in a Mat object is done with the \lstinline|Mat imread(const String &filename,int flags)| function. The input image gets converted into a specific color space and data type depending on which flag is used. The most used flags in the presented program are \lstinline|IMREAD_GRAYSCALE| which converts a input image to a single channel grayscale \lstinline|Mat| object with a bit size of eight bit and \lstinline|IMREAD_COLOR| which converts the input image into a 3 channel \lstinline|Mat| where every channel has a bit size of 8 bit. Figure \ref{fig:imread} shows an example image loaded with both two different flags.
Accessing the single pixel values in \textit{OpenCV} can be done using pointers or using the \lstinline|Mat.at<interal type>(j,i)| method if the obtained image has only one channel.

\newpage

\section{Image filters and edge detectors}
\label{chapter::imagefilteredgedetector}
\begin{figure}[h]
\centering
\includegraphics[height=4.5cm]{pictures/basics/filtersandstuff/convolution.png}
\caption{Example of convolution kernel with a size of $3\times3$ pixel.\cite{Jean2017}}
\label{fig:convolution}
\end{figure}
In most cases the vision system is not able to process the acquired raw image data directly. The task of image filters is to enhance or modify the acquired raw data by eliminating undesirable characteristics such as random variations in intensity, poor contrast or variations in illumination\cite{Beyerer2016}. Filtering can be performed in the frequency domain by masking specific frequency regions or in the spatial domain by convolution. 

A linear spatial filter is an image operator where each pixel value $I(u,v)$ is changed by a function of the intensities of pixels in a neighborhood of $(u,v)$\cite{utah1}. The matrix of the weights is called the convolution kernel. The value $I'(u,v)$ is therefor calculated with
\begin{equation}
I'(u,v) = \sum_{k=1}^{n} \sum_{l=1}^{m} I(k,l) * g(u-k, v-l)
\end{equation}
where $g(x,y)$ is a weight at the location $(x,y)$ of the convolution mask and $n$/$m$ its amount of columns/rows.
However close to the border of the image where the filter mask ranges over the edge some points need to be extrapolated, otherwise the resulting image gets smaller. 

The theoretical right way to cope with this problem is called \textit{cyclic convolution}. This means that the convolution is repeated periodically so if the kernel arrives at the left border, the missing points should be taken from the right edge of the image\cite{Jahne2005}. This approach is not often used in practical application, instead the border is extended with the half of the width of the filter mask. These border values can be set using different approaches. A simple approach is to fill the border area with zeros. This will introduce horizontal and vertical edges to the image. Another simple approach is to write the same values of the edge pixels into the border area. The image can also be extend by reflection or by replicating pixels from the other side. 

\begin{figure}[h]
\centering
  \subfloat[original image]{%
    \includegraphics[width=.15\textwidth]{pictures/basics/filtersandstuff/Athene.png}}\hfill
  \subfloat[Constant zero border]{%
    \includegraphics[width=.15\textwidth]{pictures/basics/filtersandstuff/border_constant.png}}\hfill
  \subfloat[Reflecting border]{%
    \includegraphics[width=.15\textwidth]{pictures/basics/filtersandstuff/border_reflect.png}}\hfill
  \subfloat[Replicating border]{%
    \includegraphics[width=.15\textwidth]{pictures/basics/filtersandstuff/border_wrap.png}}\hfill
  \caption{Different border types applied on an input image $(a)$.  }\label{Logical squares}

\end{figure}

\newpage

\subsection{Blur filters}
\textit{Smoothing} or \textit{blurring} filters are used to remove details and noise from images. They are commonly applied on an image before its edges are detected\cite{Jahne2005}\cite{OpenCVBlur}.
\begin{figure}[h]
\centering
  \subfloat[Input image]{%
    \includegraphics[width=.15\textwidth]{pictures/basics/filtersandstuff/Lenna.png}}\hfill
  \subfloat[Gaussian blur]{%
    \includegraphics[width=.15\textwidth]{pictures/basics/filtersandstuff/lenna_gauss.png}}\hfill
  \subfloat[Box blur]{%
    \includegraphics[width=.15\textwidth]{pictures/basics/filtersandstuff/lenna_average.png}}\hfill
  \caption{Image smoothing with Gaussian and Box filter with a reflected border}\label{Logical squares}
  \end{figure}

One of the simplest linear blurring filters is a so called \textit{Box blur}. The filter represents a form of low-pass filter in which each pixel of the resulting image has a value equal to the average of its neighborhood pixel. The kernel of a $3\times3$ \textit{Box blur} filter looks like
\begin{equation}
(1/9)*
\begin{bmatrix}
  1 & 1 & 1 \\ 
  1 & 1 & 1 \\
  1 & 1 & 1
\end{bmatrix}
\end{equation}
and can be computed in \textit{OpenCV} with the \lstinline|blur()| function.
The most common used blurring filter used is the so called \textit{Gaussian blur}. Therefore the Gaussain 2D function 
\begin{equation}
G(x,y) = \frac{1}{2\pi\sigma^2}e^{-\frac{x^2}{2\sigma^2}}
\end{equation}
where $x$ is the distance from the origin in the horizontal axis, $y$ is the distance from the origin in the vertical axis and $\sigma$ represent the standard deviation of the Gaussian distribution, gets approximated with a kernel. The bigger the size of the kernel the more the image gets blurred. An example of a multivariate Gaussian function with a standard deviation of one and its approximated kernel is shown below. 
\begin{figure}[h]
\centering
\begin{minipage}[h]{0.5\textwidth}
\includegraphics[width=\linewidth,keepaspectratio=true]{pictures/basics/filtersandstuff/gauss2.png}
\end{minipage}%%%%%%%%%%%%%%%%
\begin{minipage}[h]{0.5\textwidth}
$(1/237)*
\begin{bmatrix}
 1 & 4 & 7 & 4 & 1 \\
  4 & 16 & 26 & 16 & 4 \\
   7 & 26 & 41 & 26 & 7 \\
    4 & 16 & 26 & 16 & 4 \\
     1 & 4 & 7 & 4 & 1      
\end{bmatrix}
$
\end{minipage}
\caption{A 3D illustration of the 2D Gaussian function with a standard deviation of one(left) and its approximated $5\times5$ sized kernel(right)}
\end{figure}

\newpage

\subsection{Edge detectors}
\begin{figure}[h]
\centering
  \subfloat[Roberts Cross]{%
    \includegraphics[width=.22\textwidth]{pictures/basics/filtersandstuff/robertscross.png}}\hfill
  \subfloat[Sobel]{%
    \includegraphics[width=.22\textwidth]{pictures/basics/filtersandstuff/sobel.png}}\hfill
  \subfloat[LoG]{%
    \includegraphics[width=.22\textwidth]{pictures/basics/filtersandstuff/LoG.png}}\hfill
   \subfloat[Canny]{%
        \includegraphics[width=.22\textwidth]{pictures/basics/filtersandstuff/canny.png}}
  \caption{Inverted output of different edge detectors. The input image is "Lenna", the same as in figure \ref{Logical squares}\label{Logical squares}}
\end{figure}
Edge detectors are used to identify features in an image which belong to the structure and properties of objects in the scene. Edges typically occur on the boundary between two different regions in an image. In an ideal case the result of applying an edge detector to an image lead to the boundaries of an object. Several edge detectors were developed in the past but it still remains an active research field due to its importance for feature detection and feature extraction.\cite{Beyerer2016}\cite{Gonzalez2002}. Three simple edge detectors are presented in the following. 
\paragraph{Roberts cross}
One of the first edge detectors was presented by Lawrence Roberts which approximates the gradient of an image through discrete differentiation.\cite{Roberts1965a}. Therefore the difference between diagonally adjacent pixel is calculated using two different kernels and their sum of squares is build. The gradient is defined as 
\begin{align}
\begin{aligned}
G(x,y) &= \sqrt{G_x^2 + G_y^2}  & 
\Theta &= \arctan{(\frac{G_y(x,y)}{G_x(x,y})} - \frac{3\pi}{4} &
G_x &= \begin{bmatrix} 1  & 0 \\ 0  & -1 \end{bmatrix} & 
G_y &= \begin{bmatrix} 0  & 1 \\ -1 & 0 \end{bmatrix}
\end{aligned}
\end{align}

Applying this detector on an image takes three steps. First both kernels $G_x$ and $G_y$  need to be extended $G_x$ and $G_y$ to a $3\times3$ kernel by adding an additional zero column/row at the beginning of the matrix. Convolving the image with the first kernel finds the gradient edges at -45\textdegree  and the second at 45\textdegree. The two resulting images are combined to a resulting image with their sum of squares. The benefit of this edge detector is its easy implementation however it is very sensitive to noise\cite{Davis1975a}.
\paragraph{Sobel}
Another detector which also detects edges based on the approximation of the derivatives in x and y-direction is the Sobel filter\cite{Sobel1990}. The filter is defines as 
\begin{align}
G(x,y) &= \sqrt{G_x^2 + G_y^2}  &
\Theta &= \arctan{(\frac{G_y(x,y)}{G_x(x,y})} &
G_x &= \begin{bmatrix} 1  & 0 & -1 \\ 2 & 0 & -2 \\ 1  & 0 & -1 \end{bmatrix} &
G_y &= \begin{bmatrix} 1  & 2 & 1 \\ 0 & 0 & 0 \\ -1  & -2 & -1 \end{bmatrix} &
\end{align} 
The $G_x$ kernel approximates the derivative along the x-axis and the $G_y$ kernel approximates the derivative along the y-axis. The calculation process of the resulting gradient image is also done by convolution. Due to its larger kernel the Sobel operator takes a bit more computational time but therefore smooths the input image to a greater extend which makes the operator less sensitive to noise. 
\paragraph{Laplacian/LoG}
The Laplacian operator is a convolution kernel which approximates the second derivate in both directions and adds them up. The definition of the Laplacian $L(x,y)$ is defined as 
\begin{align}
L(x,y) &= \frac{\delta^2I}{\delta^2x} + \frac{\delta^2I}{\delta^2y} &
K_1 &= \begin{bmatrix} 0  & 1 & 0 \\ 1 & -4 & 1 \\ 0  & 1 &  0 \end{bmatrix} &
K_2 &= \begin{bmatrix} 1  & 1 & 1 \\ 1 & -8 & 1 \\ 1  & 1 &  1 \end{bmatrix} 
\end{align}
with $K_1$ and $K_2$ are two commonly used approximations. Using the second derivative makes the operator very sensitive to noise. Thats why typically the image is first blurred with a Gaussian filter. These two steps can be merged together resulting in the \textit{Laplacian of Gausian}(Log) kernel. The LoG function for two dimensions is defined as 
\begin{align}
LoG(x,y) &= -\frac{1}{\pi\sigma^4} \left[ 1-\frac{x^2+y^2}{2\sigma^2} \right] e^{-\frac{x^2+y^2}{2\sigma^2}}
\label{eq:LoG}
\end{align}
a discrete approximation with a kernel size of $9\times9$ and $\sigma=1.4$ is listed at the appendix. 

\paragraph{Canny edge detector}
Looking for pixel correspondences between the camera and the display single pixels are needed in order to retrieve pixel-to pixel correspondences.
With the previously mentioned operators the edges are represented with the gradient,  which results in thick edges which need to be thinned. A thinning algorithm is presented in chapter\ref{chap:pointmatching} which thins the edges without any information about its orientation. However if the orientation of the edge is taken into account canny offers a more precise thinning with additional noise suppression. 
The canny edge detector is an algorithm which improves the edge detecting with the previously mentioned operators. Therefore the algorithm uses 6 steps. 

In the first step the image is smoothened with the Gaussian filter. 
Then the gradient of the image is detected using the Sobel operator where the magnitude of the gradient is approximated as $|G| = |G_x| + |G_y|$. 
The orientation of the edge is calculated with $\Theta = \arctan2(G_y G_x)$ where there exist 4 possible solutions for an edge point to be in a digital image. The first position would be on a horizontal edge so its orientation would be zero. Other possible positions of the edge point on the edge would be along the positive/negative diagonal which represents a orientation of 45 respectively 135 degree and along on a vertical edge which represents an orientation of 90 degree. So the orientation values are mapped to 0, 45, 90 or 135 if they lie in a certain interval. 

With the known directions of the edges non maximum suppression is applied to thin the edges resolving into single pixel edges. 
The non maximum-suppression algorithm has two steps which are applied on each pixel of the gradient image. First the edge strength of the current pixel is compare with the strength of the pixels in the negative and positive direction of the gradient. The value of the current pixel will be suppressed if its strength is lower then the pixel in the mask with the same direction otherwise it will be preserved. 

Two binarized images are then created using a upper and a lower threshold. So the image with the higher threshold has less noise and fewer false edges but therefore bigger gaps between edge segments compared to the image with a lower threshold.

The last step is to decide which edges of the two binearized images are preserved in the output image called is hysteresis. It is obvious that the edge points found in the image with the higher threshold should be preserved because they represent strong edges. The additional edge points found in the image with the lower threshold are only preserved if one of its eight connected neighbors belongs to a strong edge.

Because of its high accuracy and the fact that the algorithm is already implemented as a function in the \textit{OpenCV} library the canny edge detector is chosen in the created program. 